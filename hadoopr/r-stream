#!/usr/bin/env python

## Copyright 2013 Elliot Chow

## Licensed under the Apache License, Version 2.0 (the "License")
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at

## http://www.apache.org/licenses/LICENSE-2.0

## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.


import os
import sys
import re
import subprocess
from optparse import OptionParser
import getpass

if __name__ == '__main__':
  script_dir = os.path.dirname(sys.argv[0])

  parser = OptionParser()

  parser.add_option("-i", dest = "input",
                    help="input directory")

  parser.add_option("-o", dest = "output",
                    help="output directory")

  parser.add_option("-w", dest = "workspace",
                    default = ".",
                    help="workspace containing mapper.R and (possibly) reducer.R")

  parser.add_option("--hargs", dest = "hadoop_args",
                    default = '',
                    help="hadoop arguments (e.g. -Dmapred.job.queue.name=my-queue, -Dmapred.reduce.tasks.speculative.execution=true)")

  parser.add_option("-x", dest = "extra_files",
                    default = 'extra files to include',
                    help="hadoop arguments")

  parser.add_option("--overwrite", action = 'store_true', dest = "overwrite",
                    help="hadoop arguments")

  parser.add_option("--exclude-moRe", action = 'store_true', dest = "exclude_moRe",
                    help="excluded moRe")

  parser.add_option("-p", action = 'store_true', dest = "print_only",
                    help="only hadoop print command")

  parser.add_option("--R", dest = "r_location",
                    default = "/user/%s/R-2.15.3-build.tar.gz" % getpass.getuser(),
                    help="path to R build")

  (options, args) = parser.parse_args()

  if not options.input: raise Exception('-i is required')
  if not options.output: raise Exception('-o is required')

  try: hadoop_home = os.environ['HADOOP_HOME']
  except Exception as e: raise Exception('HADOOP_HOME not set')

  try: streaming_jar = subprocess.check_output("find -L %s -name '*streaming*jar'" % hadoop_home)
  except Exception as e: raise Exception('failed find streaming jar in %s' % hadoop_home)

  mapper_path = os.path.join(options.workspace, 'mapper.R')
  if not os.path.exists(mapper_path): raise Exception('mapper.R is missing in workspace %s' % options.workspace)

  reducer_path = os.path.join(options.workspace, 'reducer.R')
  reducer_path = reducer_path if os.path.exists(reducer_path) else None

  moRe_files = map(lambda p: os.path.join(script_dir, '..', p), filter(lambda p: re.match(r'.*[.]R$',p), os.listdir(os.path.join(script_dir, '..')) if not options.exclude_moRe else []))
  extra_files = re.split(r'\s+', options.extra_files)
  files = map(lambda p: '-file %s' % p, filter(lambda x: x and os.path.isfile(x), moRe_files + [os.path.join(script_dir, 'mapper.sh'), os.path.join(script_dir, 'reducer.sh'), os.path.join(script_dir, 'run-mapper.R'), os.path.join(script_dir, 'run-reducer.R'), mapper_path, reducer_path] + extra_files))

  run_job_cmd = '%(hadoop_home)s/bin/hadoop jar %(streaming_jar)s %(hadoop_args)s -cacheArchive %(r_location)s#R-build -input %(input)s -output %(output)s %(files)s -mapper %(mapper)s -reducer %(reducer)s' % {'hadoop_home': hadoop_home, 'streaming_jar': streaming_jar, 'hadoop_args': options.hadoop_args, 'r_location': options.r_location, 'input': options.input, 'output': options.output, 'files': ' '.join(files), 'mapper': './mapper.sh', 'reducer': './reducer.sh' if reducer_path else "NONE"}

  print run_job_cmd

  if not options.print_only:
    hadoop_job_process = subprocess.call(run_job_cmd, shell=True)

